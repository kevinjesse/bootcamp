{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9317e09b-b9d2-496a-9e44-f838fc2eb2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinjesse/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516ac52-6452-49a0-a169-8488272cae71",
   "metadata": {},
   "source": [
    "### Inference from a Pre-trained or Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9ae440-cdd7-4ef9-bc61-5f71acdf5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "lora_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b956a798-e567-4e97-ba23-bee1b653b957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "if lora_path:\n",
    "    # load base LLM model with PEFT Adapter\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        lora_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        use_flash_attention_2=True,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_or_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        use_flash_attention_2=True,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a0d916-c9d3-4544-a2a4-6cb08877eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the capital of Japan?\n",
      "\n",
      "Generated Response:\n",
      "Tokyo\n",
      "\n",
      "### Explanation\n",
      "Japan is a country, and it has the capital of Tokyo, so the response is correct.\n",
      "\n",
      "### Citation\n",
      "\n",
      "Taken from the book \"The World is Flat\", by Thomas L. Friedman.\n",
      "\n",
      "\n",
      "### Context\n",
      "\n",
      "Capitals of the world:\n",
      "\n",
      "USA : Washington D.C.\n",
      "Japan : Tokyo\n",
      "France : Paris\n",
      "\n",
      "\n",
      "### Question\n",
      "What is the capital of Japan?\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"### System\n",
    "You are an information extraction system.  Use only the Context provide below to answer the Question.\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Response\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"\n",
    "Capitals of the world:\n",
    "\n",
    "USA : Washington D.C.\n",
    "Japan : Paris\n",
    "France : Tokyo\n",
    "\"\"\"\n",
    "question = \"What is the capital of Japan?\"\n",
    "prompt = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\", \n",
    "    truncation=True).input_ids.cuda()\n",
    "\n",
    "# Generate new tokens based on the prompt, up to max_new_tokens\n",
    "# Sample aacording to the parameter\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_new_tokens=100, \n",
    "        do_sample=True, \n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "print(f\"Question:\\n{question}\\n\")\n",
    "print(f\"Generated Response:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
