{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Sentiment predictor across IMDB Reviews\n",
    "\n",
    "In this project, we fine-tune an LSTM-based model for sentiment analysis on the IMDB movie review dataset. The dataset consists of 50,000 movie reviews, each labeled as either positive or negative, making it an ideal binary classification task. We split the data into training, validation, and test sets, using 80% for training and the remaining 20% for validation, while a separate test set evaluates the model's performance after training.\n",
    "\n",
    "A key aspect of our approach is the use of pre-trained GloVe embeddings. GloVe embeddings capture semantic relationships between words, having been trained on large corpora to represent words as dense vectors. By using these pre-trained embeddings, we leverage linguistic knowledge that helps our model understand word meanings more effectively right from the start, reducing the amount of training data needed and improving generalization.\n",
    "\n",
    "During fine-tuning, we adapt the model to the specific task of sentiment classification. The LSTM processes each review as a sequence of tokens, and at the final time step, we extract the hidden state of the last token, which captures the contextual information from the entire review. This hidden state is passed through a fully connected layer and a sigmoid activation function to output a probability, which represents the model's confidence that the review has a positive sentiment. We then apply a threshold (0.5 by default) to classify the review as either positive or negative based on this probability.\n",
    "\n",
    "The final product is a fine-tuned LSTM model capable of analyzing the sentiment of new, unseen movie reviews, classifying them as either positive or negative. This model could be further deployed for tasks such as customer feedback analysis or opinion mining in different domains, where understanding sentiment is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Importing Libraries**\n",
    "- **`numpy as np`**: Used for numerical operations like creating arrays and performing mathematical functions.\n",
    "- **`torch`**: Core PyTorch library for building and training deep learning models.\n",
    "- **`torch.nn as nn`**: Provides layers, loss functions, and utilities for neural networks.\n",
    "- **`torch.optim as optim`**: Contains optimization algorithms like `SGD` and `Adam` for model weight updates.\n",
    "- **`torch.utils.data.DataLoader` and `Dataset`**: Manage batching, shuffling, and parallel data loading.\n",
    "- **`datasets.load_dataset`**: From Hugging Face, used to load datasets (e.g., IMDB, MNIST).\n",
    "- **`os`**: Interacts with the operating system (file and directory management).\n",
    "- **`pickle`**: Serializes and deserializes objects, useful for saving/loading models or data.\n",
    "- **`sklearn.metrics.accuracy_score`**: Evaluates model accuracy based on predictions.\n",
    "- **`tqdm`**: Displays progress bars for loops, useful for tracking training or data processing tasks.\n",
    "- **`torch.utils.tensorboard.SummaryWriter`**: Logs data to TensorBoard, helping to visualize metrics like loss and accuracy during training.\n",
    "\n",
    "### 2. **Setting the Device**\n",
    "- The code checks if a **GPU** (CUDA) is available. If so, it sets the device to **GPU**, otherwise, it defaults to **CPU**.\n",
    "- **`cuda`**: Refers to Nvidia GPUs, which can accelerate model training.\n",
    "- **`cpu`**: The fallback device when no GPU is available.\n",
    "\n",
    "### 3. **Printing the Device in Use**\n",
    "- Verifies and prints which device (GPU or CPU) is being used, ensuring the model runs on the correct hardware.\n",
    "\n",
    "### Summary\n",
    "- The code is setting up for deep learning model execution using PyTorch.\n",
    "- It loads essential libraries for building, training, and evaluating the model.\n",
    "- It ensures that computations run on the best available hardware, be it a GPU or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Function Definition**\n",
    "- **`initialize_tokenizer`**: This function creates or loads a tokenizer, which maps words to indices and vice versa. Tokenizers are essential in NLP for converting text data into numerical format that can be fed into machine learning models.\n",
    "\n",
    "### 2. **Parameters**\n",
    "- **`tokenizer_type='word'`**: Defines the type of tokenizer (e.g., word-level). Though this parameter is not used in the current function, it allows future extension (for tokenizing by character, subword, etc.).\n",
    "- **`text_data=None`**: Expects a list of text data that the tokenizer will be built from. If this is `None`, there is no input data to create the tokenizer.\n",
    "- **`min_freq=1`**: Filters out words that appear less frequently than the specified minimum frequency (`min_freq`). This is useful to remove rare words that may not provide much value in modeling.\n",
    "- **`max_vocab_size=None`**: Limits the tokenizer to a maximum vocabulary size. If this is set, only the `max_vocab_size` most frequent words will be kept.\n",
    "- **`tokenizer_file='tokenizer.pkl'`**: Specifies the filename where the tokenizer will be saved or loaded from. If this file exists, the function loads the tokenizer from this file.\n",
    "\n",
    "### 3. **Loading Existing Tokenizer**\n",
    "- **`os.path.exists(tokenizer_file)`**: Checks if a tokenizer file already exists. If it does, it loads the tokenizer from the file using `pickle`.\n",
    "  - **`with open(tokenizer_file, 'rb') as f:`**: Opens the tokenizer file in binary read mode and loads it.\n",
    "  - **`tokenizer['word2idx']`**: A dictionary that maps words to their indices (e.g., `{'cat': 1}`).\n",
    "  - **`vocab_size`**: The size of the vocabulary (i.e., the number of unique words in the tokenizer).\n",
    "\n",
    "### 4. **Building a New Tokenizer**\n",
    "- If the tokenizer file does not exist, the function builds a new tokenizer from the `text_data`:\n",
    "  - **`tokenizer = {'word2idx': {'<UNK>': 0}, 'idx2word': {0: '<UNK>'}}`**: Initializes the tokenizer with a special token `'<UNK>'` for unknown words. It creates two dictionaries:\n",
    "    - `word2idx`: Maps words to indices.\n",
    "    - `idx2word`: Maps indices back to words.\n",
    "  - **`word_counts = {}`**: Initializes a dictionary to store word frequencies.\n",
    "\n",
    "### 5. **Counting Word Frequencies**\n",
    "- The function iterates over `text_data`, splits each text into words, and counts the frequency of each word. This helps identify the most common words to be added to the tokenizer:\n",
    "  - **`word_counts[word] = word_counts.get(word, 0) + 1`**: Increments the word count for each word in the text.\n",
    "\n",
    "### 6. **Filtering Words by Frequency**\n",
    "- **`if count >= min_freq`**: Filters out words that occur less than the `min_freq`.\n",
    "  \n",
    "### 7. **Sorting and Limiting Vocabulary**\n",
    "- **`sorted(words, key=lambda x: x[1], reverse=True)`**: Sorts the words by their frequency in descending order so that the most frequent words are added to the tokenizer first.\n",
    "- **`if max_vocab_size is not None`**: If `max_vocab_size` is specified, the vocabulary is limited to that many words, accounting for the `'<UNK>'` token.\n",
    "\n",
    "### 8. **Adding Words to the Tokenizer**\n",
    "- The loop **`for word, _ in words:`** adds each word and its corresponding index to the `word2idx` and `idx2word` dictionaries:\n",
    "  - **`tokenizer['word2idx'][word] = idx`**: Assigns an index to each word.\n",
    "  - **`tokenizer['idx2word'][idx] = word`**: Adds the reverse mapping of index to word.\n",
    "\n",
    "### 9. **Saving the Tokenizer**\n",
    "- **`with open(tokenizer_file, 'wb') as f:`**: Saves the tokenizer using `pickle` to a binary file for future use. This avoids rebuilding the tokenizer every time the program runs.\n",
    "\n",
    "### 10. **Returning the Tokenizer and Vocabulary Size**\n",
    "- The function returns the `tokenizer` dictionary and the size of the vocabulary.\n",
    "\n",
    "### Summary\n",
    "- This function efficiently builds or loads a tokenizer that converts words into indices. It allows setting a minimum word frequency and a maximum vocabulary size to filter out rare words and limit the vocabulary size. The tokenizer can be saved and loaded from a file to avoid unnecessary recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize tokenizer with a max vocabulary size\n",
    "def initialize_tokenizer(tokenizer_type='word', text_data=None, min_freq=1, max_vocab_size=None, tokenizer_file='tokenizer.pkl'):\n",
    "    if os.path.exists(tokenizer_file):\n",
    "        print(\"Loading tokenizer from file...\")\n",
    "        with open(tokenizer_file, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        vocab_size = len(tokenizer['word2idx'])\n",
    "    else:\n",
    "        print(\"Building tokenizer...\")\n",
    "        tokenizer = {'word2idx': {'<UNK>': 0}, 'idx2word': {0: '<UNK>'}}\n",
    "        idx = 1\n",
    "        word_counts = {}\n",
    "\n",
    "        # Count the frequency of each word\n",
    "        for text in text_data:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Filter words by minimum frequency\n",
    "        words = [(word, count) for word, count in word_counts.items() if count >= min_freq]\n",
    "\n",
    "        # Sort words by frequency in descending order and limit to max_vocab_size\n",
    "        words = sorted(words, key=lambda x: x[1], reverse=True)\n",
    "        if max_vocab_size is not None:\n",
    "            words = words[:max_vocab_size - 1]  # Subtract 1 to account for <UNK>\n",
    "        \n",
    "        # Add words to the tokenizer\n",
    "        for word, _ in words:\n",
    "            tokenizer['word2idx'][word] = idx\n",
    "            tokenizer['idx2word'][idx] = word\n",
    "            idx += 1\n",
    "\n",
    "        vocab_size = idx\n",
    "\n",
    "        # Save the tokenizer to a file\n",
    "        with open(tokenizer_file, 'wb') as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "    \n",
    "    return tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Function Definition**\n",
    "- **`load_glove_embeddings`**: This function loads GloVe word embeddings and maps them to the vocabulary created earlier using `word2idx`. GloVe embeddings are pre-trained word vectors that capture semantic relationships between words.\n",
    "\n",
    "### 2. **Parameters**\n",
    "- **`glove_path`**: Path to the GloVe embeddings file (e.g., a `.txt` file containing word vectors).\n",
    "- **`word2idx`**: A dictionary mapping words to their corresponding indices in the tokenizer.\n",
    "- **`embedding_dim`**: The dimensionality of the word vectors (e.g., 50, 100, 300), which defines the size of the embedding space.\n",
    "- **`embeddings_file='embeddings.pt'`**: Specifies the filename where the embeddings will be saved or loaded from, using PyTorch's format (`.pt`).\n",
    "\n",
    "### 3. **Loading Existing Embeddings**\n",
    "- **`if os.path.exists(embeddings_file):`**: Checks if the embeddings file already exists. If so, it loads the pre-saved embeddings from the file to save time.\n",
    "  - **`torch.load(embeddings_file)`**: Loads the saved embeddings in a PyTorch tensor format from the specified file. This avoids having to reprocess the GloVe embeddings every time.\n",
    "\n",
    "### 4. **Generating New Embeddings**\n",
    "- If the embeddings file does not exist, the function generates new embeddings:\n",
    "  - **`embeddings = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))`**: Initializes an embedding matrix where each word in the vocabulary is represented by a random vector. The random vectors follow a normal distribution with a standard deviation of 0.6.\n",
    "  - **`with open(glove_path, 'r', encoding='utf-8') as f:`**: Opens the GloVe file for reading. This file contains words and their corresponding pre-trained embeddings.\n",
    "\n",
    "### 5. **Processing Each Line of GloVe Embeddings**\n",
    "- **`for line in f:`**: Iterates over each line in the GloVe file. Each line represents a word followed by its embedding vector.\n",
    "  - **`values = line.split()`**: Splits the line into individual components. The first component is the word, and the rest are the values of its embedding vector.\n",
    "  - **`word = values[0]`**: The word itself.\n",
    "  - **`vector = np.asarray(values[1:], dtype='float32')`**: Converts the rest of the line (the embedding vector values) into a NumPy array.\n",
    "  - **`if word in word2idx:`**: Checks if the word from the GloVe file is in the `word2idx` dictionary (i.e., if it's part of the vocabulary). If it is, the corresponding vector is assigned to the appropriate index in the embedding matrix:\n",
    "    - **`embeddings[word2idx[word]] = vector`**: Assigns the GloVe embedding vector to the correct position in the embeddings matrix.\n",
    "\n",
    "### 6. **Converting to PyTorch Tensor**\n",
    "- **`torch.tensor(embeddings, dtype=torch.float)`**: Converts the NumPy embedding matrix into a PyTorch tensor, which is required for deep learning models in PyTorch.\n",
    "- **`torch.save(embeddings, embeddings_file)`**: Saves the embeddings to a file for future use, avoiding reprocessing.\n",
    "\n",
    "### 7. **Returning Embeddings**\n",
    "- The function returns the final embeddings matrix in the form of a PyTorch tensor.\n",
    "\n",
    "### Summary\n",
    "- This function either loads pre-trained GloVe embeddings from a file or generates new embeddings if the file does not exist.\n",
    "- If generating new embeddings, it initializes a random embedding matrix, then replaces the random vectors with the pre-trained GloVe embeddings for any words in the vocabulary (`word2idx`).\n",
    "- The embeddings are saved in a PyTorch tensor format, making them easy to load and use in subsequent training tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings (from previous script)\n",
    "def load_glove_embeddings(glove_path, word2idx, embedding_dim, embeddings_file='embeddings.pt'):\n",
    "    vocab_size = len(word2idx)\n",
    "    if os.path.exists(embeddings_file):\n",
    "        print(\"Loading embeddings from file...\")\n",
    "        embeddings = torch.load(embeddings_file)\n",
    "    else:\n",
    "        print(\"Generating embeddings...\")\n",
    "        embeddings = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                if word in word2idx:\n",
    "                    embeddings[word2idx[word]] = vector\n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float)\n",
    "        torch.save(embeddings, embeddings_file)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Class Definition**\n",
    "- **`SentimentDataset`**: This class is a custom PyTorch dataset tailored for sentiment analysis tasks. It inherits from `torch.utils.data.Dataset`, allowing PyTorch to manage it like any other dataset (for batching, shuffling, etc.).\n",
    "\n",
    "### 2. **`__init__` Method**\n",
    "- **`texts`**: A list of text data (e.g., movie reviews) that will be used for sentiment analysis.\n",
    "- **`labels`**: A list of labels corresponding to the texts, indicating sentiment (e.g., positive or negative).\n",
    "- **`tokenizer`**: A dictionary containing mappings from words to indices (`word2idx`) created in earlier steps. It is used to convert the text data into a sequence of indices.\n",
    "- **`sequence_length`**: The maximum number of tokens per text. Texts longer than this will be truncated, and shorter ones will be padded.\n",
    "- **`self.word2idx`**: Extracts the `word2idx` dictionary from the tokenizer and stores it for easier access.\n",
    "\n",
    "### 3. **`__len__` Method**\n",
    "- **`return len(self.texts)`**: Returns the number of texts in the dataset. This helps PyTorch understand how many samples are in the dataset, which is necessary for batching and iteration.\n",
    "\n",
    "### 4. **`__getitem__` Method**\n",
    "This method retrieves a specific data sample based on the provided index (`idx`). It's essential for PyTorch's data loading process.\n",
    "\n",
    "- **`text = self.texts[idx]`**: Retrieves the text at the given index `idx`.\n",
    "- **`label = self.labels[idx]`**: Retrieves the corresponding label for that text (e.g., 0 for negative, 1 for positive sentiment).\n",
    "\n",
    "### 5. **Tokenizing the Text**\n",
    "- **`tokens = [self.word2idx.get(word, 0) for word in text.strip().split()]`**: This line splits the text into individual words and converts each word into its corresponding index using the `word2idx` dictionary. If a word is not found, the default index `0` is used, which typically represents the `<UNK>` (unknown) token.\n",
    "  \n",
    "### 6. **Handling Sequence Length**\n",
    "- **`tokens = tokens[:self.sequence_length]`**: Truncates the list of tokens if it exceeds the specified `sequence_length`, ensuring that all inputs are of uniform length.\n",
    "- **Padding**: \n",
    "  - **`if len(tokens) < self.sequence_length:`**: If the number of tokens is less than the `sequence_length`, the text is padded with zeros (index `0`) at the beginning. This ensures that all token sequences are of the same length.\n",
    "  - **`tokens = [0] * (self.sequence_length - len(tokens)) + tokens`**: Adds the necessary number of padding tokens (zeros) to the front of the sequence.\n",
    "\n",
    "### 7. **Returning Tensors**\n",
    "- **`torch.tensor(tokens, dtype=torch.long)`**: Converts the tokenized and padded sequence into a PyTorch tensor of type `long`, which is typically used for categorical data such as word indices.\n",
    "- **`torch.tensor(label, dtype=torch.float)`**: Converts the label into a PyTorch tensor of type `float`, which is useful for binary classification tasks (such as sentiment analysis).\n",
    "\n",
    "### Summary\n",
    "- **`SentimentDataset`** is a custom PyTorch dataset class for sentiment analysis. It processes a list of texts by tokenizing and padding them, then returns them as tensors. The labels are also returned as tensors, which are useful for training a model in PyTorch.\n",
    "- The class ensures that all sequences are of uniform length, either by truncating or padding them, which is essential for batch processing in deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class for sentiment analysis\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, sequence_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.word2idx = tokenizer['word2idx']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = [self.word2idx.get(word, 0) for word in text.strip().split()]\n",
    "        tokens = tokens[:self.sequence_length]\n",
    "        if len(tokens) < self.sequence_length:\n",
    "            tokens = [0] * (self.sequence_length - len(tokens)) + tokens\n",
    "\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Class Definition**\n",
    "- **`LSTMSentimentModel`**: This class defines an LSTM-based neural network model for sentiment analysis. It inherits from `nn.Module`, the base class for all neural networks in PyTorch.\n",
    "\n",
    "### 2. **`__init__` Method**\n",
    "This method initializes the layers and components of the model.\n",
    "\n",
    "- **`input_size`**: The size of the input vocabulary (i.e., the number of unique tokens in the tokenizer). This is the number of words that can be represented in the embedding layer.\n",
    "- **`embedding_dim`**: The size of each word embedding vector (e.g., 100 or 300 dimensions).\n",
    "- **`hidden_size`**: The number of units in the hidden layers of the LSTM. This controls the capacity of the model to learn from sequential data.\n",
    "- **`output_size`**: The number of output units. Typically, for binary sentiment analysis, this is `1` (representing the probability of positive/negative sentiment).\n",
    "- **`num_layers=2`**: The number of stacked LSTM layers. Stacking multiple LSTM layers allows the model to learn more complex patterns.\n",
    "- **`dropout=0.3`**: Dropout rate, used to prevent overfitting by randomly dropping some neurons during training.\n",
    "- **`embedding_weights=None`**: If pre-trained word embeddings (e.g., GloVe) are provided, they are used to initialize the embedding layer.\n",
    "- **`bidirectional=False`**: If `True`, the LSTM will be bidirectional, meaning it will process the input sequence in both forward and backward directions.\n",
    "\n",
    "#### Embedding Layer\n",
    "- **`self.embedding = nn.Embedding(input_size, embedding_dim)`**: This layer maps the input words (represented by their indices) to dense vectors of size `embedding_dim`.\n",
    "- **`if embedding_weights is not None:`**: If pre-trained embeddings are provided, they are loaded into the embedding layer.\n",
    "  - **`self.embedding.weight.data.copy_(embedding_weights)`**: Copies the pre-trained embedding weights (e.g., GloVe) into the embedding layer.\n",
    "\n",
    "#### LSTM Layer\n",
    "- **`self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)`**:\n",
    "  - **`embedding_dim`**: The input to the LSTM is the embedding of each word.\n",
    "  - **`hidden_size`**: The LSTM's hidden state size, controlling the model's memory capacity.\n",
    "  - **`num_layers=num_layers`**: Specifies the number of LSTM layers stacked.\n",
    "  - **`batch_first=True`**: Ensures that the batch dimension is the first dimension in the input (`batch_size, sequence_length, embedding_dim`).\n",
    "  - **`bidirectional=bidirectional`**: If `True`, the LSTM will process the sequence in both forward and backward directions.\n",
    "\n",
    "#### Dropout Layer\n",
    "- **`self.dropout = nn.Dropout(dropout)`**: Adds dropout regularization to prevent overfitting by randomly dropping some units during training.\n",
    "\n",
    "#### Fully Connected Layer\n",
    "- **`self.fc = nn.Linear(hidden_size, output_size)`**: A fully connected layer that maps the final hidden state of the LSTM to the output (the predicted sentiment). If the LSTM is bidirectional, `hidden_size` is doubled.\n",
    "\n",
    "### 3. **`forward` Method**\n",
    "This method defines the forward pass of the model, where input data passes through the layers.\n",
    "\n",
    "- **Embedding Layer**: \n",
    "  - **`x = self.embedding(x)`**: Converts the input word indices into dense vectors (embeddings).\n",
    "  \n",
    "- **LSTM Layer**:\n",
    "  - **`x, _ = self.lstm(x)`**: Processes the embeddings through the LSTM. The LSTM outputs a sequence of hidden states (one for each time step in the input sequence).\n",
    "  \n",
    "- **Dropout**:\n",
    "  - **`x = self.dropout(x)`**: Applies dropout to the LSTM outputs to regularize the model.\n",
    "  \n",
    "- **Fully Connected Layer**:\n",
    "  - **`x = self.fc(x[:, -1, :])`**: Only the last time step of the LSTM's output is passed to the fully connected layer. This represents the final hidden state, which is used for sentiment prediction.\n",
    "\n",
    "- **Sigmoid Activation**:\n",
    "  - **`torch.sigmoid(x)`**: The output of the fully connected layer is passed through a sigmoid activation function to map the output to a probability between 0 and 1, useful for binary classification.\n",
    "  - **`.squeeze()`**: Removes any extra dimensions from the output.\n",
    "\n",
    "### Summary\n",
    "- The model consists of an embedding layer (which can use pre-trained embeddings), followed by a stacked LSTM for sequence processing, and a dropout layer for regularization. A fully connected layer generates the final prediction, and a sigmoid activation is applied to produce the output as a probability of positive/negative sentiment.\n",
    "- If bidirectional LSTM is enabled, it will process the sequence in both forward and backward directions, improving the model's ability to understand context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model for sentiment analysis\n",
    "class LSTMSentimentModel(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, num_layers=2, dropout=0.3, embedding_weights=None, bidirectional=False):\n",
    "        super(LSTMSentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_weights)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if bidirectional:\n",
    "            hidden_size *= 2\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use only the last output of the LSTM\n",
    "        return torch.sigmoid(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Function Definition**\n",
    "- **`save_checkpoint`**: This function saves the model's state (a checkpoint) to a file during training, typically after every epoch. If the current model is the best so far (based on some evaluation metric), it also updates the \"best\" model checkpoint.\n",
    "\n",
    "### 2. **Parameters**\n",
    "- **`state`**: A dictionary containing the current state of the model and optimizer, typically including model weights, optimizer states, epoch number, and any other relevant information you wish to save.\n",
    "- **`is_best`**: A boolean flag indicating whether the current model is the best model so far (based on validation performance, for example). If `True`, the function saves an additional checkpoint labeled as the \"best\" model.\n",
    "- **`checkpoint_dir=\"checkpoints\"`**: The directory where all model checkpoints will be stored. If this directory does not exist, the function creates it.\n",
    "- **`best_model_name=\"sentiment_model_best.pt\"`**: The filename used to save the best model checkpoint.\n",
    "\n",
    "### 3. **Creating the Checkpoint Directory**\n",
    "- **`if not os.path.exists(checkpoint_dir):`**: Checks if the directory for storing checkpoints exists. If not:\n",
    "  - **`os.makedirs(checkpoint_dir)`**: Creates the directory to store checkpoints.\n",
    "\n",
    "### 4. **Saving the Epoch Checkpoint**\n",
    "- **`epoch_checkpoint = os.path.join(checkpoint_dir, f'sentiment_model_epoch_{state[\"epoch\"]}.pt')`**: Creates a filename for saving the checkpoint. The filename includes the current epoch number, allowing the model state to be saved after each epoch.\n",
    "- **`torch.save(state, epoch_checkpoint)`**: Saves the `state` dictionary to the specified file. This is the primary function for saving model checkpoints in PyTorch.\n",
    "\n",
    "### 5. **Updating the Best Model Checkpoint**\n",
    "- **`if is_best:`**: If the current model is the best so far (i.e., if `is_best` is `True`), the function saves it as the \"best\" model.\n",
    "  - **`best_checkpoint = os.path.join(checkpoint_dir, best_model_name)`**: Creates the path for the best model checkpoint.\n",
    "  - **`torch.save(state, best_checkpoint)`**: Saves the current state as the best model checkpoint.\n",
    "  - This ensures that the best model is updated and can be retrieved later even if subsequent models (from later epochs) perform worse.\n",
    "\n",
    "### 6. **Printing Checkpoint Status**\n",
    "- **`print(f\"Checkpoint saved: {epoch_checkpoint}\")`**: Notifies that a checkpoint for the current epoch has been saved.\n",
    "- **`print(f\"Best checkpoint updated: {best_checkpoint}\")`**: Notifies that the best model checkpoint has been updated.\n",
    "\n",
    "### Summary\n",
    "- This function saves the model's state after each epoch to track the model's progress over time.\n",
    "- If the model performs better than any previous versions (based on some external evaluation), it also updates a separate file to store the \"best\" model.\n",
    "- The function ensures the checkpoints are organized and easy to retrieve for later use or to resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model checkpoint\n",
    "def save_checkpoint(state, is_best, checkpoint_dir=\"checkpoints\", best_model_name=\"sentiment_model_best.pt\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    epoch_checkpoint = os.path.join(checkpoint_dir, f'sentiment_model_epoch_{state[\"epoch\"]}.pt')\n",
    "    torch.save(state, epoch_checkpoint)\n",
    "    print(f\"Checkpoint saved: {epoch_checkpoint}\")\n",
    "    \n",
    "    if is_best:\n",
    "        best_checkpoint = os.path.join(checkpoint_dir, best_model_name)\n",
    "        torch.save(state, best_checkpoint)\n",
    "        print(f\"Best checkpoint updated: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Function Definition**\n",
    "- **`get_model`**: This function initializes a model for sentiment analysis based on the input parameters. It either loads a pre-trained model or initializes a model with random weights depending on the `pretrained` flag.\n",
    "\n",
    "### 2. **Parameters**\n",
    "- **`vocab_size`**: The size of the vocabulary (i.e., the number of unique tokens). This is used for the embedding layer, where each word is represented by a vector.\n",
    "- **`embedding_dim`**: The dimensionality of the word embeddings (e.g., 100 or 300). This defines the size of the embedding space.\n",
    "- **`hidden_size`**: The number of hidden units in the LSTM layers. A higher hidden size allows the model to capture more complex patterns but also increases computation.\n",
    "- **`num_layers`**: The number of stacked LSTM layers. Stacking multiple layers allows the model to learn more hierarchical features from the data.\n",
    "- **`dropout`**: The dropout rate applied to the LSTM layers, which helps prevent overfitting by randomly dropping units during training.\n",
    "- **`pretrained=False`**: A flag indicating whether to load a pre-trained model or to initialize the model with random weights. If `True`, it attempts to use `pretrained_weights` to initialize the embedding layer.\n",
    "- **`pretrained_weights=None`**: If `pretrained` is `True`, this parameter holds the pre-trained word embeddings (e.g., GloVe) that will be used to initialize the embedding layer.\n",
    "\n",
    "### 3. **Condition for Pre-trained Model**\n",
    "- **`if pretrained and pretrained_weights is not None:`**: This checks whether the user wants to use a pre-trained model and whether pre-trained weights have been provided.\n",
    "  - **`print(\"Loading pre-trained model...\")`**: Notifies that a pre-trained model is being loaded.\n",
    "  - **`LSTMSentimentModel(..., embedding_weights=pretrained_weights)`**: Initializes the `LSTMSentimentModel` with the pre-trained embedding weights. The embeddings will not be randomly initialized but instead will be copied from `pretrained_weights`.\n",
    "\n",
    "### 4. **Random Initialization**\n",
    "- **`else:`**: If `pretrained` is `False` or no pre-trained weights are provided, the model will be randomly initialized.\n",
    "  - **`print(\"Randomly initializing model...\")`**: Notifies that the model is being initialized with random weights.\n",
    "  - **`LSTMSentimentModel(..., embedding_weights=None)`**: Initializes the model with random embeddings, as no pre-trained weights are provided.\n",
    "\n",
    "### 5. **Return the Model**\n",
    "- The function returns the constructed model, which can either be initialized with pre-trained weights or with random weights, depending on the parameters.\n",
    "\n",
    "### Summary\n",
    "- The `get_model` function provides flexibility in model initialization. It can either load a pre-trained model (if `pretrained=True` and `pretrained_weights` are provided) or randomly initialize a new model. This allows you to leverage pre-trained embeddings like GloVe or FastText if available, or train from scratch if desired.\n",
    "- The use of hyperparameters such as `vocab_size`, `embedding_dim`, `hidden_size`, `num_layers`, and `dropout` allows you to easily configure and adjust the model architecture based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or initialize model based on a hyperparameter\n",
    "def get_model(vocab_size, embedding_dim, hidden_size, num_layers, dropout, pretrained=False, pretrained_weights=None):\n",
    "    if pretrained and pretrained_weights is not None:\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        model = LSTMSentimentModel(vocab_size, embedding_dim, hidden_size, 1, num_layers=num_layers, dropout=dropout, embedding_weights=pretrained_weights)\n",
    "    else:\n",
    "        print(\"Randomly initializing model...\")\n",
    "        model = LSTMSentimentModel(vocab_size, embedding_dim, hidden_size, 1, num_layers=num_layers, dropout=dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Function Definition**\n",
    "- **`evaluate_model`**: This function evaluates the performance of a model on a validation dataset. It calculates the loss and accuracy by passing the validation data through the model without updating the weights.\n",
    "\n",
    "### 2. **Parameters**\n",
    "- **`model`**: The LSTM model (or any neural network model) that is being evaluated.\n",
    "- **`data_loader`**: A PyTorch `DataLoader` containing the validation data. The data loader batches the data for evaluation.\n",
    "- **`criterion`**: The loss function used to calculate how well the model's predictions match the actual labels. Common criteria include `nn.BCELoss` or `nn.CrossEntropyLoss`.\n",
    "\n",
    "### 3. **Model in Evaluation Mode**\n",
    "- **`model.eval()`**: This sets the model to evaluation mode, disabling dropout and batch normalization, which are only needed during training.\n",
    "- **`with torch.no_grad():`**: This context manager ensures that no gradients are computed during evaluation, saving memory and computation time since the model is not being updated.\n",
    "\n",
    "### 4. **Looping Through the Validation Data**\n",
    "- **`for texts, labels in data_loader:`**: Iterates through batches of data (texts and corresponding labels) from the data loader.\n",
    "  - **`texts, labels = texts.to(device), labels.to(device)`**: Moves the texts and labels to the correct device (GPU or CPU) for processing.\n",
    "\n",
    "### 5. **Model Predictions**\n",
    "- **`outputs = model(texts)`**: Passes the texts (inputs) through the model to obtain the outputs (predictions). These outputs are usually probabilities between 0 and 1, especially if using a sigmoid activation for binary classification.\n",
    "\n",
    "### 6. **Loss Calculation**\n",
    "- **`loss = criterion(outputs, labels)`**: Calculates the loss for the current batch using the loss function (`criterion`). The loss measures how far off the model's predictions are from the true labels.\n",
    "- **`total_loss += loss.item()`**: Adds the loss for the current batch to the total loss.\n",
    "\n",
    "### 7. **Converting Outputs to Predictions**\n",
    "- **`preds = (outputs >= 0.5).float()`**: Converts the model's outputs to binary predictions (either 0 or 1). If the output is greater than or equal to 0.5, it is considered a positive prediction (1); otherwise, it is negative (0).\n",
    "  - **`predictions.extend(preds.cpu().numpy())`**: Adds the predicted values for the current batch to the list of all predictions. The `.cpu().numpy()` ensures that the predictions are moved to CPU and converted to NumPy arrays for compatibility with evaluation functions.\n",
    "  - **`true_labels.extend(labels.cpu().numpy())`**: Adds the true labels for the current batch to the list of all true labels.\n",
    "\n",
    "### 8. **Accuracy Calculation**\n",
    "- **`accuracy = accuracy_score(true_labels, predictions)`**: Uses `sklearn.metrics.accuracy_score` to compute the accuracy by comparing the model's predictions with the true labels.\n",
    "- **`print(f\"Accuracy: {accuracy:.4f}\")`**: Prints the accuracy of the model on the validation dataset.\n",
    "\n",
    "### 9. **Average Validation Loss**\n",
    "- **`avg_val_loss = total_loss / len(data_loader)`**: Computes the average validation loss by dividing the total loss by the number of batches.\n",
    "- **`print(f\"Validation Loss: {avg_val_loss:.4f}\")`**: Prints the average validation loss.\n",
    "\n",
    "### 10. **Return Values**\n",
    "- **`return avg_val_loss, accuracy`**: Returns both the average validation loss and the accuracy, which can be used for further analysis or logging.\n",
    "\n",
    "### Summary\n",
    "- **`evaluate_model`** computes both the validation loss and accuracy of the model without updating the weights (i.e., in evaluation mode).\n",
    "- It processes the validation data in batches, calculates the loss and predictions, and evaluates the model's performance by comparing its predictions to the true labels.\n",
    "- The function outputs the average validation loss and accuracy, which are essential metrics to monitor when assessing a model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function to compute the validation loss\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    avg_val_loss = total_loss / len(data_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return avg_val_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Function Definition**\n",
    "- **`train_model`**: This function trains the model over multiple epochs, saves checkpoints after each epoch, and logs training and validation metrics such as loss and accuracy. \n",
    "\n",
    "### 2. **Parameters**\n",
    "- **`model`**: The LSTM model (or any neural network model) being trained.\n",
    "- **`train_loader`**: A `DataLoader` containing the training data, which provides batches of texts and labels for training.\n",
    "- **`val_loader`**: A `DataLoader` containing the validation data for evaluating the model after each epoch.\n",
    "- **`criterion`**: The loss function used to calculate the training loss (e.g., `nn.BCELoss` for binary classification).\n",
    "- **`optimizer`**: The optimizer (e.g., `Adam`, `SGD`) used to update the model's parameters during training.\n",
    "- **`num_epochs=5`**: The number of epochs (full passes through the training data) for which the model will be trained.\n",
    "- **`writer=None`**: A `SummaryWriter` object for logging to TensorBoard. This logs metrics like loss and accuracy so they can be visualized in TensorBoard.\n",
    "\n",
    "### 3. **Tracking Best Validation Loss**\n",
    "- **`best_val_loss = float('inf')`**: Initializes the best validation loss as infinity. This is used to keep track of the best model based on validation loss. If the current validation loss is lower than `best_val_loss`, the model is saved as the best model.\n",
    "\n",
    "### 4. **Epoch Loop**\n",
    "- **`for epoch in range(num_epochs):`**: Loops over the specified number of epochs. Each epoch represents one full pass over the training data.\n",
    "  - **`model.train()`**: Puts the model in training mode, enabling dropout and batch normalization layers (if present).\n",
    "\n",
    "### 5. **Training Loop**\n",
    "- **`for texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):`**: Loops over batches of training data provided by `train_loader`. `tqdm` is used to show a progress bar for each epoch.\n",
    "  - **`texts, labels = texts.to(device), labels.to(device)`**: Moves the input data (texts and labels) to the appropriate device (GPU or CPU).\n",
    "  - **`optimizer.zero_grad()`**: Resets the gradients for the optimizer before backpropagation.\n",
    "  - **`outputs = model(texts)`**: Passes the batch of texts through the model to get the predictions (outputs).\n",
    "  - **`loss = criterion(outputs, labels)`**: Computes the loss for the current batch by comparing the outputs and labels using the specified loss function.\n",
    "  - **`loss.backward()`**: Performs backpropagation, computing the gradients for each parameter based on the loss.\n",
    "  - **`nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)`**: Clips the gradients to prevent exploding gradients, ensuring the maximum norm of the gradients does not exceed 1.\n",
    "  - **`optimizer.step()`**: Updates the model parameters using the gradients calculated during backpropagation.\n",
    "\n",
    "### 6. **Tracking and Logging Training Loss**\n",
    "- **`total_loss += loss.item()`**: Accumulates the training loss for the current epoch.\n",
    "- **`avg_train_loss = total_loss / len(train_loader)`**: Computes the average training loss for the current epoch.\n",
    "- **`writer.add_scalar('Loss/Train', avg_train_loss, epoch + 1)`**: Logs the average training loss to TensorBoard for the current epoch.\n",
    "\n",
    "### 7. **Evaluating on Validation Set**\n",
    "- **`avg_val_loss, accuracy_score = evaluate_model(model, val_loader, criterion)`**: After each epoch, the model is evaluated on the validation set using the `evaluate_model` function, which returns the average validation loss and accuracy.\n",
    "- **`writer.add_scalar('Loss/Validation', avg_val_loss, epoch + 1)`**: Logs the validation loss to TensorBoard for the current epoch.\n",
    "- **`writer.add_scalar('Accuracy/Validation', accuracy_score, epoch + 1)`**: Logs the validation accuracy to TensorBoard for the current epoch.\n",
    "\n",
    "### 8. **Saving Checkpoints**\n",
    "- **`is_best = avg_val_loss < best_val_loss`**: Checks if the current epoch's validation loss is the best so far (i.e., lower than `best_val_loss`).\n",
    "- **`if is_best:`**: If the current model has the best validation loss, update the best validation loss and save the model checkpoint as the best model.\n",
    "  - **`best_val_loss = avg_val_loss`**: Updates the `best_val_loss` to the current epoch's validation loss.\n",
    "\n",
    "- **`checkpoint_state`**: A dictionary that stores the current state of the model, including:\n",
    "  - `epoch`: The current epoch number.\n",
    "  - `model_state_dict`: The state of the model (weights).\n",
    "  - `optimizer_state_dict`: The state of the optimizer.\n",
    "  - `loss`: The average training loss for the current epoch.\n",
    "  - `val_loss`: The average validation loss for the current epoch.\n",
    "\n",
    "- **`save_checkpoint(checkpoint_state, is_best)`**: Saves a checkpoint for the current epoch, and if it's the best model so far, it also updates the \"best model\" checkpoint.\n",
    "\n",
    "### Summary\n",
    "- **`train_model`** trains the model for a specified number of epochs while tracking and logging training and validation metrics.\n",
    "- During each epoch, the model parameters are updated based on the training data, and the model is evaluated on the validation data.\n",
    "- Checkpoints are saved at the end of each epoch, and the model with the best validation loss is flagged as the \"best model\" checkpoint. The use of TensorBoard logging allows for easy visualization of training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with checkpoint saving\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, writer=None):\n",
    "    best_val_loss = float('inf')  # Track best validation loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the gradients to prevent exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Log the train loss to TensorBoard\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch + 1)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        avg_val_loss, accuracy_score = evaluate_model(model, val_loader, criterion)\n",
    "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch + 1)\n",
    "        writer.add_scalar('Accuracy/Validation', accuracy_score, epoch + 1)\n",
    "\n",
    "        # Save a checkpoint for every epoch\n",
    "        is_best = avg_val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = avg_val_loss\n",
    "        \n",
    "        checkpoint_state = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }\n",
    "        \n",
    "        save_checkpoint(checkpoint_state, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Main Function Definition**\n",
    "- **`main(pretrained=False)`**: This is the main function that sets up the model, loads the data, and trains the model for sentiment analysis. The `pretrained` flag determines whether to use pre-trained word embeddings (e.g., GloVe) or initialize the embeddings randomly.\n",
    "\n",
    "### 2. **Hyperparameters**\n",
    "- **`embedding_dim = 50`**: The dimensionality of the word embeddings (each word will be represented as a vector of 50 dimensions).\n",
    "- **`hidden_size = 256`**: The number of units in the LSTM's hidden layers.\n",
    "- **`num_layers = 2`**: The number of LSTM layers to stack.\n",
    "- **`dropout = 0.05`**: The dropout rate for regularization.\n",
    "- **`sequence_length = 512`**: The maximum length of each input sequence (texts longer than this will be truncated, and shorter ones padded).\n",
    "- **`batch_size = 1024`**: The number of samples processed together in each batch during training.\n",
    "- **`num_epochs = 20`**: The number of epochs (full passes through the training data).\n",
    "\n",
    "### 3. **Initialize TensorBoard Writer**\n",
    "- **`writer = SummaryWriter()`**: Initializes the TensorBoard writer to log training and validation metrics, allowing visualization of the training process.\n",
    "\n",
    "### 4. **Load IMDB Dataset**\n",
    "- **`load_dataset('imdb')`**: Loads the IMDB dataset using the Hugging Face `datasets` library, which contains a large collection of movie reviews labeled as positive or negative.\n",
    "- **`train_dataset` and `test_dataset`**: The dataset is split into a training set (`train_dataset`) and a test set (`test_dataset`).\n",
    "\n",
    "### 5. **Train/Validation Split**\n",
    "- **`train_val_split = train_dataset.train_test_split(test_size=0.2)`**: Splits the training dataset into 80% training and 20% validation sets. The validation set is used to evaluate the model after each epoch.\n",
    "- **`train_texts`, `train_labels`, `val_texts`, `val_labels`**: These variables store the texts and corresponding labels for the training and validation sets.\n",
    "  \n",
    "### 6. **Initialize Tokenizer**\n",
    "- **`initialize_tokenizer`**: Initializes a tokenizer to convert the input texts into sequences of word indices. It ensures the vocabulary is capped at 32,000 words, with a minimum word frequency of 5.\n",
    "- **`vocab_size`**: The size of the tokenizer's vocabulary, which will be used for the embedding layer.\n",
    "\n",
    "### 7. **Loading GloVe Embeddings (if `pretrained=True`)**\n",
    "- **`if pretrained:`**: If pre-trained embeddings are required, it loads the GloVe embeddings from a file and maps them to the words in the tokenizer using the function `load_glove_embeddings`.\n",
    "- **`glove_path = './glove.6B.50d.txt'`**: Path to the GloVe embeddings file with 50-dimensional word vectors.\n",
    "\n",
    "### 8. **Create Dataset and DataLoaders**\n",
    "- **`SentimentDataset`**: Creates instances of the `SentimentDataset` class for the training, validation, and test datasets. This class converts the texts into sequences of word indices and pads/truncates them to the specified `sequence_length`.\n",
    "- **`DataLoader`**: PyTorch's `DataLoader` is used to load batches of data efficiently:\n",
    "  - **`train_loader`**: Loads batches of training data with shuffling enabled.\n",
    "  - **`val_loader`**: Loads validation data without shuffling.\n",
    "  - **`test_loader`**: Loads test data without shuffling for evaluation after training.\n",
    "\n",
    "### 9. **Initialize or Load Model**\n",
    "- **`get_model`**: Initializes the LSTM model for sentiment analysis. If `pretrained=True`, the embedding layer will be initialized with pre-trained embeddings; otherwise, random embeddings will be used.\n",
    "\n",
    "### 10. **Loss Function and Optimizer**\n",
    "- **`criterion = nn.BCELoss()`**: The binary cross-entropy loss function is used, as this is a binary classification task (positive vs. negative sentiment).\n",
    "- **`optimizer = optim.Adam(model.parameters(), lr=0.001)`**: The Adam optimizer is used for updating the model's parameters with a learning rate of 0.001.\n",
    "\n",
    "### 11. **Train the Model**\n",
    "- **`train_model`**: The function trains the model over the specified number of epochs (`num_epochs`). It logs training and validation losses to TensorBoard and saves model checkpoints after each epoch.\n",
    "\n",
    "### 12. **Close TensorBoard Writer**\n",
    "- **`writer.close()`**: Closes the TensorBoard writer after training is complete to ensure all data is properly written.\n",
    "\n",
    "### 13. **Evaluate on the Full Test Set**\n",
    "- **`evaluate_model`**: After training, the model is evaluated on the entire test dataset to determine its final performance (validation loss and accuracy).\n",
    "\n",
    "### 14. **Main Execution**\n",
    "- **`if __name__ == '__main__':`**: This ensures the script runs when executed directly. By setting `pretrained=False`, the model will be initialized with random embeddings. If you set `pretrained=True`, GloVe embeddings will be used.\n",
    "\n",
    "### Summary\n",
    "- This script trains an LSTM model for sentiment analysis using the IMDB dataset.\n",
    "- The model can optionally use pre-trained GloVe embeddings for word representations.\n",
    "- Training and validation metrics (loss, accuracy) are logged to TensorBoard for visualization.\n",
    "- After training, the model is evaluated on a separate test set to assess its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pretrained=False):\n",
    "    # Hyperparameters\n",
    "    embedding_dim = 50\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    dropout = 0.05\n",
    "    sequence_length = 512\n",
    "    batch_size = 1024\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Load training dataset\n",
    "    dataset = load_dataset('imdb')\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    # Split train set into 80% train, 20% validation\n",
    "    train_val_split = train_dataset.train_test_split(test_size=0.2)\n",
    "    train_texts = train_val_split['train']['text']\n",
    "    train_labels = train_val_split['train']['label']\n",
    "    val_texts = train_val_split['test']['text']\n",
    "    val_labels = train_val_split['test']['label']\n",
    "    \n",
    "    test_texts = test_dataset['text']\n",
    "    test_labels = test_dataset['label']\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer, vocab_size = initialize_tokenizer(\n",
    "        tokenizer_type='word', \n",
    "        text_data=train_texts, \n",
    "        min_freq=5, \n",
    "        max_vocab_size=32000,  # Cap the vocabulary at 10,000 words\n",
    "        tokenizer_file='tokenizer.pkl'\n",
    "    )\n",
    "\n",
    "    # Load GloVe embeddings if pretrained=True\n",
    "    if pretrained:\n",
    "        glove_path = './glove.6B.50d.txt'\n",
    "        embeddings = load_glove_embeddings(glove_path, tokenizer['word2idx'], embedding_dim)\n",
    "    else:\n",
    "        embeddings = None\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    train_data = SentimentDataset(train_texts, train_labels, tokenizer, sequence_length)\n",
    "    val_data = SentimentDataset(val_texts, val_labels, tokenizer, sequence_length)\n",
    "    test_data = SentimentDataset(test_texts, test_labels, tokenizer, sequence_length)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize or load model\n",
    "    model = get_model(vocab_size, embedding_dim, hidden_size, num_layers, dropout, pretrained, embeddings).to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, writer)\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # Evaluate the model on the full test set\n",
    "    print(\"\\nEvaluating on full test set...\")\n",
    "    evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set to True to load pretrained embeddings, False for random initialization\n",
    "    main(pretrained=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
