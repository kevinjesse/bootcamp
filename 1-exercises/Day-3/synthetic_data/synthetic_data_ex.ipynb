{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "input_file = \"data/fake_data_systems_tables.csv\",\n",
    "data = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns from the dataset\n",
    "original_columns = data.columns.tolist()\n",
    " \n",
    "# New columns for the generated table structure\n",
    "generated_columns = ['ColumnNames', 'ColumnAcronyms', 'DataTypes', 'Nullability', 'ColumnDescriptions']\n",
    " \n",
    "# Initialize an empty dataframe to store the results\n",
    "# Note: We adjust the DataFrame initialization to better accommodate list storage for generated columns\n",
    "results_df = pd.DataFrame()\n",
    " \n",
    "results_df.head(10)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data for efficiency, comment out the line below if you want to go through the full dataset\n",
    "data = data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "# ADD YOUR HUGGINGFACE LOGIN TOKEN HERE\n",
    "login(token = \"API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME YOU RUN THIS, IT MIGHT TAKE A WHILE\n",
    "\n",
    "model_path_or_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "def generate(prompt):\n",
    "    \"\"\"Convenience function for generating model output\"\"\"\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True).input_ids.cuda()\n",
    "    \n",
    "    # Generate new tokens based on the prompt, up to max_new_tokens\n",
    "    # Sample according to the parameter\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_new_tokens=1000, \n",
    "            do_sample=True, \n",
    "            top_p=0.9,\n",
    "            temperature=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_table_structure(info):\n",
    "    \"\"\"Generates a prompt for creating synthetic table structures.\"\"\"\n",
    "    prompt = f\"\"\"Please generate a table with 20 synthetic column names, column acronyms, data types, nullability, and column descriptions for the given table information: {info}.\n",
    "    Provide the output in CSV format.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def parse_table_structure(table_text):\n",
    "    \"\"\"Parses the generated table structure into a list of dictionaries.\"\"\"\n",
    "    lines = table_text.strip().split('\\n')\n",
    "    table_data = []\n",
    "    for line in lines:\n",
    "        fields = line.split(',')\n",
    "        if len(fields) == 5:  # Assuming the structure includes 5 items: column name, acronym, type, nullability, and description\n",
    "            table_data.append({\n",
    "                \"Column Name\": fields[0].strip(),\n",
    "                \"Acronym\": fields[1].strip(),\n",
    "                \"Data Type\": fields[2].strip(),\n",
    "                \"Nullability\": fields[3].strip(),\n",
    "                \"Description\": fields[4].strip()\n",
    "            })\n",
    "    return table_data\n",
    "\n",
    "# Initialize a list to hold prompts\n",
    "prompts = data.apply(lambda row: generate_table_structure(row.to_json()), axis=1)\n",
    "\n",
    "# Convert prompts to list for processing\n",
    "prompts_list = list(prompts.values)\n",
    "\n",
    "# Prepare a list to store all generated tables\n",
    "generated_tables = []\n",
    "\n",
    "# Iterate over each prompt, invoke the model, and parse the generated tables\n",
    "for idx, prompt in enumerate(prompts_list):\n",
    "    if idx >= len(data):  # Prevent index error by checking bounds\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # Call the model to generate output for each prompt\n",
    "    output = generate(prompt)\n",
    "    # Extract the text from the output (since it's a string)\n",
    "    table_text = output.strip()\n",
    "    parsed_table = parse_table_structure(table_text)\n",
    "\n",
    "    # Extract original data\n",
    "    table_name = data.iloc[idx]['Table Name']\n",
    "    table_description = data.iloc[idx]['Table Description']\n",
    "\n",
    "    # Add the parsed table to the generated tables list\n",
    "    generated_tables.append({\n",
    "        'Table Name': table_name,\n",
    "        'Table Description': table_description,\n",
    "        'Generated Table': parsed_table\n",
    "    })\n",
    "\n",
    "# Now prepare the final DataFrame with all the generated tables\n",
    "output_data = []\n",
    "for idx, table_info in enumerate(generated_tables):\n",
    "    for row in table_info['Generated Table']:\n",
    "        output_data.append({\n",
    "            'Source System Name': data.iloc[idx]['Source System Name'],\n",
    "            'Source System Acronym': data.iloc[idx]['Source System Acronym'],\n",
    "            'Table Name': table_info['Table Name'],\n",
    "            'Table Description': table_info['Table Description'],\n",
    "            'Generated_ColumnNames': row['Column Name'],\n",
    "            'Generated_ColumnAcronyms': row['Acronym'],\n",
    "            'Generated_DataTypes': row['Data Type'],\n",
    "            'Generated_Nullability': row['Nullability'],\n",
    "            'Generated_ColumnDescriptions': row['Description']\n",
    "        })\n",
    "\n",
    "# Convert the output data into a DataFrame\n",
    "final_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'data/generated_table_structures_with_original_data.csv'\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
